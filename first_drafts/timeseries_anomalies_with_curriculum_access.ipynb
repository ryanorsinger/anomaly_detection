{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from numpy import linspace, loadtxt, ones, convolve\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from random import randint\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def evaluate(actual, predictions, output=True):\n",
    "    mse = metrics.mean_squared_error(actual, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    if output:\n",
    "        print('MSE:  {}'.format(mse))\n",
    "        print('RMSE: {}'.format(rmse))\n",
    "    else:\n",
    "        return mse, rmse    \n",
    "\n",
    "def plot_and_eval(predictions, actual, metric_fmt='{:.2f}', linewidth=4):\n",
    "    if type(predictions) is not list:\n",
    "        predictions = [predictions]\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(train,label='Train')\n",
    "    plt.plot(test, label='Test')\n",
    "\n",
    "    for yhat in predictions:\n",
    "        mse, rmse = evaluate(actual, yhat, output=False)        \n",
    "        label = f'{yhat.name}'\n",
    "        if len(predictions) > 1:\n",
    "            label = f'{label} -- MSE: {metric_fmt} RMSE: {metric_fmt}'.format(mse, rmse)\n",
    "        plt.plot(yhat, label=label, linewidth=linewidth)\n",
    "\n",
    "    if len(predictions) == 1:\n",
    "        label = f'{label} -- MSE: {metric_fmt} RMSE: {metric_fmt}'.format(mse, rmse)\n",
    "        plt.title(label)\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['timestamp', 'resource', 'user', 'cohort_id', 'ip_address']\n",
    "\n",
    "df = pd.read_csv(\"./data/curriculum-access.txt\", sep='\\t', header=None,\n",
    "                 index_col=False,\n",
    "                 names=colnames)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.user.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.user.value_counts().tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
